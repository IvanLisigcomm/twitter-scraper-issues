# ðŸ“– ä½¿ç”¨ç¤ºä¾‹

## ç¤ºä¾‹1ï¼šçˆ¬å–Elon Muskçš„æŽ¨æ–‡

### Webç•Œé¢æ“ä½œ
1. å¯åŠ¨æœåŠ¡ï¼š`./start_web.sh`
2. è®¿é—®ï¼šhttp://localhost:8887
3. å¡«å†™è¡¨å•ï¼š
   - ç”¨æˆ·åï¼š`elonmusk`
   - æ•°é‡ï¼š`50`
   - æ ¼å¼ï¼š`csv`ï¼ˆé»˜è®¤ï¼‰
   - æ— å¤´æ¨¡å¼ï¼š`âœ“`
4. ç‚¹å‡»"å¼€å§‹çˆ¬å–"
5. ç­‰å¾…è¿›åº¦å®Œæˆ
6. ç‚¹å‡»"é¢„è§ˆæ•°æ®"æŸ¥çœ‹ç»“æžœï¼ˆæ”¯æŒCSVé¢„è§ˆï¼‰
7. ç‚¹å‡»"ä¸‹è½½æ–‡ä»¶"ä¿å­˜åˆ°æœ¬åœ°

### é¢„æœŸç»“æžœ
- ç”Ÿæˆæ–‡ä»¶ï¼š`elonmusk_20251001_120000.json`
- ç”Ÿæˆæ–‡ä»¶ï¼š`elonmusk_20251001_120000.csv`
- åŒ…å«æœ€æ–°50æ¡æŽ¨æ–‡
- åŒ…å«ç‚¹èµžã€è½¬å‘ã€è¯„è®ºæ•°

## ç¤ºä¾‹2ï¼šæ‰¹é‡çˆ¬å–å¤šä¸ªç”¨æˆ·

### æ–¹æ³•Aï¼šWebç•Œé¢è¿žç»­æ“ä½œ
1. çˆ¬å–ç”¨æˆ·A
2. ç­‰å¾…å®Œæˆ
3. ç‚¹å‡»"æ–°å»ºä»»åŠ¡"
4. çˆ¬å–ç”¨æˆ·B
5. é‡å¤æ­¥éª¤

### æ–¹æ³•Bï¼šå‘½ä»¤è¡Œè„šæœ¬
```bash
# åˆ›å»ºæ‰¹é‡è„šæœ¬
cat > batch_scrape.py << 'EOF'
from twitter_scraper import TwitterScraper

users = ['elonmusk', 'BillGates', 'BarackObama']
scraper = TwitterScraper(headless=True)

for user in users:
    print(f"å¼€å§‹çˆ¬å– {user}...")
    tweets = scraper.scrape_user_tweets(user, max_tweets=30)
    scraper.save_to_json()
    scraper.save_to_csv()
    print(f"{user} å®Œæˆï¼")

scraper.close()
EOF

python3 batch_scrape.py
```

## ç¤ºä¾‹3ï¼šæŸ¥çœ‹åŽ†å²æ•°æ®

### Webç•Œé¢
1. è®¿é—®é¦–é¡µ
2. æ»šåŠ¨åˆ°"åŽ†å²è®°å½•"åŒºåŸŸ
3. ç‚¹å‡»åˆ·æ–°æŒ‰é’®
4. æŸ¥çœ‹æ‰€æœ‰å·²çˆ¬å–çš„æ–‡ä»¶
5. ç‚¹å‡»"é¢„è§ˆ"æŸ¥çœ‹å†…å®¹
6. ç‚¹å‡»"ä¸‹è½½"èŽ·å–æ–‡ä»¶

### å‘½ä»¤è¡Œ
```bash
# æŸ¥çœ‹æ‰€æœ‰JSONæ–‡ä»¶
ls -lh data/*.json

# ä½¿ç”¨jqæŸ¥çœ‹JSONå†…å®¹ï¼ˆéœ€å®‰è£…jqï¼‰
cat data/elonmusk_*.json | jq '.[0]'

# åœ¨Excelä¸­æ‰“å¼€CSV
open data/elonmusk_*.csv
```

## ç¤ºä¾‹4ï¼šæ•°æ®åˆ†æž

### ä½¿ç”¨Pythonåˆ†æž
```python
import json
import pandas as pd

# è¯»å–JSONæ•°æ®
with open('data/elonmusk_20251001_120000.json', 'r', encoding='utf-8') as f:
    tweets = json.load(f)

# è½¬æ¢ä¸ºDataFrame
df = pd.DataFrame(tweets)

# åˆ†æžç»Ÿè®¡
print(f"æ€»æŽ¨æ–‡æ•°: {len(df)}")
print(f"å¹³å‡ç‚¹èµžæ•°: {df['likes'].mean():.0f}")
print(f"æœ€å—æ¬¢è¿Žçš„æŽ¨æ–‡: {df.loc[df['likes'].idxmax(), 'text'][:100]}...")

# æŒ‰æ—¶é—´åˆ†ç»„
df['timestamp'] = pd.to_datetime(df['timestamp'])
df['date'] = df['timestamp'].dt.date
daily_tweets = df.groupby('date').size()
print("\næ¯æ—¥æŽ¨æ–‡æ•°é‡:")
print(daily_tweets)
```

### ä½¿ç”¨Excelåˆ†æž
1. æ‰“å¼€CSVæ–‡ä»¶
2. ä½¿ç”¨ç­›é€‰åŠŸèƒ½
3. åˆ›å»ºæ•°æ®é€è§†è¡¨
4. ç”Ÿæˆå›¾è¡¨

## ç¤ºä¾‹5ï¼šAPIè°ƒç”¨

### ä½¿ç”¨curl
```bash
# å¼€å§‹çˆ¬å–
curl -X POST http://localhost:8887/api/scrape \
  -H "Content-Type: application/json" \
  -d '{
    "username": "elonmusk",
    "max_tweets": 20,
    "headless": true,
    "save_format": "csv"
  }'

# æŸ¥çœ‹çŠ¶æ€
curl http://localhost:8887/api/status

# èŽ·å–åŽ†å²è®°å½•
curl http://localhost:8887/api/history

# é¢„è§ˆCSVæ–‡ä»¶
curl http://localhost:8887/api/preview/elonmusk_20251001_120000.csv

# ä¸‹è½½æ–‡ä»¶
curl -O http://localhost:8887/api/download/elonmusk_20251001_120000.csv
```

### ä½¿ç”¨Python requests
```python
import requests
import time

# å¼€å§‹çˆ¬å–
response = requests.post('http://localhost:8887/api/scrape', json={
    'username': 'elonmusk',
    'max_tweets': 50,
    'headless': True,
    'save_format': 'csv'  # é»˜è®¤CSVæ ¼å¼
})

print(response.json())

# è½®è¯¢çŠ¶æ€
while True:
    status = requests.get('http://localhost:8887/api/status').json()
    print(f"è¿›åº¦: {status['progress']}% - {status['status_message']}")
    
    if not status['is_running']:
        print("å®Œæˆï¼")
        break
    
    time.sleep(2)

# é¢„è§ˆå’Œä¸‹è½½æ–‡ä»¶
for file in status['output_files']:
    filename = file['name']
    
    # é¢„è§ˆæ–‡ä»¶ï¼ˆæ”¯æŒCSVå’ŒJSONï¼‰
    preview = requests.get(f'http://localhost:8887/api/preview/{filename}').json()
    print(f"é¢„è§ˆ {filename}: {preview['total']}æ¡æŽ¨æ–‡")
    
    # ä¸‹è½½æ–‡ä»¶
    with open(filename, 'wb') as f:
        f.write(requests.get(f'http://localhost:8887/api/download/{filename}').content)
    print(f"å·²ä¸‹è½½: {filename}")
```

## ç¤ºä¾‹6ï¼šå®šæ—¶ä»»åŠ¡

### ä½¿ç”¨cronï¼ˆLinux/Macï¼‰
```bash
# ç¼–è¾‘crontab
crontab -e

# æ¯å¤©å‡Œæ™¨2ç‚¹çˆ¬å–
0 2 * * * cd /path/to/æŽ¨ç‰¹çˆ¬è™« && python3 twitter_scraper.py < input.txt

# input.txtå†…å®¹ï¼š
# elonmusk
# 100
# y
# both
```

### ä½¿ç”¨Python schedule
```python
import schedule
import time
from twitter_scraper import TwitterScraper

def daily_scrape():
    print("å¼€å§‹æ¯æ—¥çˆ¬å–...")
    scraper = TwitterScraper(headless=True)
    tweets = scraper.scrape_user_tweets('elonmusk', max_tweets=50)
    scraper.save_to_json()
    scraper.save_to_csv()
    scraper.close()
    print("æ¯æ—¥çˆ¬å–å®Œæˆï¼")

# æ¯å¤©ä¸Šåˆ10ç‚¹æ‰§è¡Œ
schedule.every().day.at("10:00").do(daily_scrape)

print("å®šæ—¶ä»»åŠ¡å·²å¯åŠ¨...")
while True:
    schedule.run_pending()
    time.sleep(60)
```

## ç¤ºä¾‹7ï¼šé”™è¯¯å¤„ç†

### å¤„ç†ç½‘ç»œé”™è¯¯
```python
from twitter_scraper import TwitterScraper
import time

scraper = TwitterScraper(headless=True)

max_retries = 3
for attempt in range(max_retries):
    try:
        tweets = scraper.scrape_user_tweets('elonmusk', max_tweets=50)
        if tweets:
            scraper.save_to_json()
            print("æˆåŠŸï¼")
            break
    except Exception as e:
        print(f"å°è¯• {attempt + 1} å¤±è´¥: {e}")
        if attempt < max_retries - 1:
            print("ç­‰å¾…30ç§’åŽé‡è¯•...")
            time.sleep(30)
        else:
            print("æ‰€æœ‰å°è¯•å‡å¤±è´¥")

scraper.close()
```

## ç¤ºä¾‹8ï¼šè‡ªå®šä¹‰é…ç½®

### ä¿®æ”¹é»˜è®¤å‚æ•°
```python
from twitter_scraper import TwitterScraper

# è‡ªå®šä¹‰é…ç½®
scraper = TwitterScraper(
    headless=True,
    delay_range=(3, 8)  # å¢žåŠ å»¶è¿Ÿé¿å…è¢«æ£€æµ‹
)

# çˆ¬å–æ›´å¤šæŽ¨æ–‡
tweets = scraper.scrape_user_tweets(
    username='elonmusk',
    max_tweets=200  # æ›´å¤šæŽ¨æ–‡
)

scraper.save_to_json()
scraper.close()
```

## å¸¸è§é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆ

### Q1: çˆ¬å–é€Ÿåº¦æ…¢ï¼Ÿ
```python
# A: å‡å°‘å»¶è¿Ÿï¼ˆé£Žé™©æ›´é«˜ï¼‰
scraper = TwitterScraper(delay_range=(1, 3))
```

### Q2: è¢«æ£€æµ‹åˆ°ï¼Ÿ
```python
# A: å¢žåŠ å»¶è¿Ÿå’Œéšæœºæ€§
scraper = TwitterScraper(delay_range=(5, 10))
```

### Q3: å†…å­˜å ç”¨é«˜ï¼Ÿ
```python
# A: åˆ†æ‰¹å¤„ç†
for i in range(0, 500, 50):
    tweets = scraper.scrape_user_tweets('user', max_tweets=50)
    scraper.save_to_json(f'batch_{i}.json')
    scraper.tweets_data.clear()  # æ¸…ç©ºæ•°æ®
```

### Q4: éœ€è¦ä»£ç†ï¼Ÿ
```python
# A: ä¿®æ”¹ twitter_scraper.py æ·»åŠ ä»£ç†æ”¯æŒ
chrome_options.add_argument('--proxy-server=http://proxy:port')
```

## æœ€ä½³å®žè·µ

1. **ä»Žå°‘é‡å¼€å§‹**ï¼šå…ˆæµ‹è¯•10-20æ¡æŽ¨æ–‡
2. **ä½¿ç”¨æ— å¤´æ¨¡å¼**ï¼šç”Ÿäº§çŽ¯å¢ƒä½¿ç”¨æ— å¤´æ¨¡å¼
3. **åˆç†å»¶è¿Ÿ**ï¼šè®¾ç½®3-6ç§’å»¶è¿Ÿé¿å…è¢«æ£€æµ‹
4. **é”™è¯¯é‡è¯•**ï¼šæ·»åŠ é‡è¯•æœºåˆ¶å¤„ç†ç½‘ç»œé—®é¢˜
5. **å®šæœŸå¤‡ä»½**ï¼šå®šæœŸå¤‡ä»½dataç›®å½•
6. **ç›‘æŽ§æ—¥å¿—**ï¼šè®°å½•çˆ¬å–æ—¥å¿—ä¾¿äºŽè°ƒè¯•
7. **éµå®ˆè§„åˆ™**ï¼šéµå®ˆTwitteræœåŠ¡æ¡æ¬¾

---

æ›´å¤šç¤ºä¾‹å’Œé—®é¢˜ï¼Œè¯·å‚è€ƒ README.md æ–‡æ¡£ã€‚

